{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from utils import *\n",
    "from classifiers import *\n",
    "from preprocess import  preprocess\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "analyzer = 'word'\n",
    "raw = pd.read_csv('C:/Users/jordi/Documents/GitHub/MUD_Labs_Git/LangDetect/data/dataset.csv')\n",
    "voc_size = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_group_1 = {'Latin', 'Estonian', 'Indonesian', 'Swedish', 'French', 'Portuguese', 'Turkish', 'Dutch', 'English', 'Romanian', 'Spanish'}\n",
    "language_group_2 = {'Pashto', 'Urdu', 'Persian', 'Arabic'}\n",
    "language_group_3 = {'Japanese', 'Chinese', 'Thai', 'Korean', 'Tamil', 'Russian', 'Hindi'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataet in 3 groups\n",
    "raw_group_1 = pd.DataFrame(columns=['Text', 'language'])\n",
    "\n",
    "for language in language_group_1:\n",
    "    _aux_dataset = raw[raw['language'] == language]\n",
    "    raw_group_1 = pd.concat([raw_group_1, _aux_dataset], ignore_index=True)\n",
    "\n",
    "raw_group_2 = pd.DataFrame(columns=['Text', 'language'])\n",
    "\n",
    "for language in language_group_2:\n",
    "    _aux_dataset = raw[raw['language'] == language]\n",
    "    raw_group_2 = pd.concat([raw_group_2, _aux_dataset], ignore_index=True)\n",
    "\n",
    "raw_group_3 = pd.DataFrame(columns=['Text', 'language'])\n",
    "\n",
    "for language in language_group_3:\n",
    "    _aux_dataset = raw[raw['language'] == language]\n",
    "    raw_group_3 = pd.concat([raw_group_3, _aux_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train and Test sets\n",
    "_X=raw['Text']\n",
    "_y=raw['language']\n",
    "X_train, X_test, y_train, y_test = train_test_split(_X, _y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Split Train and Test sets group 1\n",
    "_X=raw_group_1['Text']\n",
    "_y=raw_group_1['language']\n",
    "X_train_g1, X_test_g1, y_train_g1, y_test_g1 = train_test_split(_X, _y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Split Train and Test sets group 2\n",
    "_X=raw_group_2['Text']\n",
    "_y=raw_group_2['language']\n",
    "X_train_g2, X_test_g2, y_train_g2, y_test_g2 = train_test_split(_X, _y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Split Train and Test sets group 3\n",
    "_X=raw_group_3['Text']\n",
    "_y=raw_group_3['language']\n",
    "X_train_g3, X_test_g3, y_train_g3, y_test_g3 = train_test_split(_X, _y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot PCA\n",
    "print('========')\n",
    "print('PCA and Explained Variance:') \n",
    "# plotPCA(X_train, X_test,y_test, languages) \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(toNumpyArray(X_train))\n",
    "pca_test = pca.transform(toNumpyArray(X_test))\n",
    "\n",
    "print('Variance explained by PCA:', pca.explained_variance_ratio_)\n",
    "\n",
    "y_test_list = np.asarray(y_test.tolist())\n",
    "for lang in languages:\n",
    "    pca_x = np.asarray([i[0] for i in pca_test])[y_test_list == lang]\n",
    "    pca_y = np.asarray([i[1] for i in pca_test])[y_test_list == lang]\n",
    "    plt.scatter(pca_x,pca_y, label=lang)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63c415ab99ec436fcb8ee002cc8458e39a88baa623caa091ec0434b54d2be80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
